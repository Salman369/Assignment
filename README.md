# Exercise 1: Basic Word Tokenization

This repository contains code for tokenizing text using the NLTK library in Python.

## Steps
1. **Tokenization**: We use `word_tokenize` to split a paragraph from *Jane Eyre* into individual words.
2. **Counting Tokens**: We count the total number of tokens.
3. **Frequency Distribution**: We use `FreqDist` to calculate the frequency of each token.

## Requirements
- Python
- NLTK library

## Usage
To run this code, open the `.py` file in your Python environment or execute the `.ipynb` notebook in Jupyter or Google Colab.
